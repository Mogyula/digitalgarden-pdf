---
{"dg-publish":true,"permalink":"/hamilton-jacobi-bellman-equation/"}
---

In optimal control theory, the Hamilton–Jacobi–Bellman (HJB) equation gives a necessary and sufficient condition for optimality of a control with respect to a loss function **in the continuous case**. Important for [[Dynamical programming\|Dynamical programming]].

It's discrete counterpart is the [[Bellman equation\|Bellman equation]].